#!/usr/bin/env python3
"""
REAL 4Runr Autonomous Organism - Uses ACTUAL Data
================================================
This organism uses REAL scraping, REAL enrichment, and REAL data.
NO MORE FAKE DATA!
"""

import os
import sys
import time
import sqlite3
import logging
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
import json

# Add paths for real components
sys.path.insert(0, os.path.join(os.getcwd(), '4runr-lead-scraper'))
sys.path.insert(0, os.path.join(os.getcwd(), '4runr-outreach-system'))

class RealAutonomousOrganism:
    def __init__(self):
        self.setup_logging()
        self.logger = logging.getLogger('real_organism')
        self.cycle_count = 0
        self.total_leads_found = 0
        self.total_leads_synced = 0
        
        # Rate limiting: 7 leads per day = 1 lead every ~3.4 hours
        self.leads_per_day = 7
        self.cycle_interval = (24 * 60 * 60) / self.leads_per_day  # ~12,343 seconds = ~3.4 hours
        
        self.logger.info(f"üß¨ REAL Autonomous Organism initialized")
        self.logger.info(f"üìä Target: {self.leads_per_day} leads per day")
        self.logger.info(f"‚è∞ Cycle interval: {self.cycle_interval/3600:.1f} hours")

    def setup_logging(self):
        """Setup comprehensive logging"""
        os.makedirs('logs', exist_ok=True)
        
        # File handler
        file_handler = logging.FileHandler('logs/real-organism.log')
        file_handler.setLevel(logging.INFO)
        
        # Console handler  
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # Root logger
        logging.basicConfig(
            level=logging.INFO,
            handlers=[file_handler, console_handler]
        )

    def validate_environment(self):
        """Validate all required environment variables and APIs"""
        required_vars = ['SERPAPI_KEY', 'AIRTABLE_API_KEY']
        missing = []
        
        for var in required_vars:
            if not os.getenv(var):
                missing.append(var)
        
        if missing:
            self.logger.error(f"‚ùå Missing environment variables: {missing}")
            return False
            
        # Test SerpAPI
        try:
            import serpapi
            client = serpapi.GoogleSearch({"q": "test", "api_key": os.getenv('SERPAPI_KEY')})
            self.logger.info("‚úÖ SerpAPI connection validated")
        except Exception as e:
            self.logger.error(f"‚ùå SerpAPI validation failed: {e}")
            return False
            
        # Test Airtable
        try:
            headers = {'Authorization': f'Bearer {os.getenv("AIRTABLE_API_KEY")}'}
            response = requests.get('https://api.airtable.com/v0/meta/bases', headers=headers)
            if response.status_code == 200:
                self.logger.info("‚úÖ Airtable connection validated")
            else:
                self.logger.error(f"‚ùå Airtable validation failed: {response.status_code}")
                return False
        except Exception as e:
            self.logger.error(f"‚ùå Airtable validation failed: {e}")
            return False
            
        return True

    def setup_database(self):
        """Ensure database exists with all required columns"""
        try:
            conn = sqlite3.connect('data/unified_leads.db')
            
            # Create table if not exists
            conn.execute('''
                CREATE TABLE IF NOT EXISTS leads (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    full_name TEXT NOT NULL,
                    email TEXT,
                    company TEXT,
                    job_title TEXT,
                    linkedin_url TEXT,
                    industry TEXT,
                    business_type TEXT,
                    source TEXT DEFAULT 'SerpAPI_Real',
                    date_scraped TEXT,
                    date_enriched TEXT,
                    date_messaged TEXT,
                    created_at TEXT,
                    enriched INTEGER DEFAULT 0,
                    ready_for_outreach INTEGER DEFAULT 0,
                    score INTEGER DEFAULT 0,
                    lead_quality TEXT,
                    email_confidence_level TEXT,
                    ai_message TEXT,
                    website TEXT,
                    needs_enrichment INTEGER DEFAULT 0,
                    replied INTEGER DEFAULT 0,
                    response_date TEXT,
                    response_notes TEXT,
                    extra_info TEXT,
                    level_engaged TEXT,
                    engagement_status TEXT,
                    follow_up_stage TEXT,
                    response_status TEXT,
                    company_description TEXT
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("‚úÖ Database schema validated")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Database setup failed: {e}")
            return False

    def scrape_real_leads(self) -> List[Dict]:
        """Use SerpAPI to find REAL LinkedIn leads"""
        try:
            import sys
            import os
            sys.path.insert(0, './4runr-lead-scraper')
            sys.path.insert(0, './4runr-lead-scraper/scraper')
            
            # Import the scraper directly
            from serpapi_scraper import SerpAPILeadScraper
            
            scraper = SerpAPILeadScraper()
            
            # Diverse search queries to find NEW leads (not same 3 people)
            import random
            
            diverse_searches = [
                'site:linkedin.com/in/ "Chief Technology Officer" Toronto',
                'site:linkedin.com/in/ "Marketing Director" Vancouver',
                'site:linkedin.com/in/ "Business Owner" Calgary', 
                'site:linkedin.com/in/ "VP Sales" Ottawa',
                'site:linkedin.com/in/ Founder startup Toronto',
                'site:linkedin.com/in/ CEO technology Vancouver',
                'site:linkedin.com/in/ Director consulting Calgary',
                'site:linkedin.com/in/ "Operations Manager" Ottawa',
                'site:linkedin.com/in/ "Product Manager" Montreal',
                'site:linkedin.com/in/ "General Manager" Toronto tech',
                'site:linkedin.com/in/ CTO fintech Vancouver',
                'site:linkedin.com/in/ CMO healthcare Calgary',
                'site:linkedin.com/in/ "Sales Director" Ottawa',
                'site:linkedin.com/in/ Founder e-commerce Montreal'
            ]
            
            # Rotate searches to get diverse results
            queries = [random.choice(diverse_searches)]
            self.logger.info(f"üéØ Using diverse search: {queries[0]}")
            
            all_leads = []
            for query in queries[:1]:  # Start with 1 query per cycle
                self.logger.info(f"üîç Searching: {query}")
                
                try:
                    # Use direct SerpAPI call with our diverse query
                    import serpapi
                    search = serpapi.GoogleSearch({
                        "q": query,
                        "api_key": os.getenv('SERPAPI_KEY'),
                        "num": 5
                    })
                    results = search.get_dict().get('organic_results', [])
                    
                    for result in results:
                        # Validate LinkedIn URL
                        linkedin_url = result.get('link', '')
                        if not self.validate_linkedin_url(linkedin_url):
                            continue
                            
                        # Extract name from title
                        title = result.get('title', '')
                        name = self.extract_name_from_linkedin(title)
                        
                        if not name or len(name) < 3:
                            continue
                            
                        lead = {
                            'full_name': name,
                            'linkedin_url': linkedin_url,
                            'job_title': self.extract_job_title(title),
                            'company': self.extract_company(title),
                            'source': 'SerpAPI_Real',
                            'date_scraped': datetime.now().isoformat(),
                            'created_at': datetime.now().isoformat()
                        }
                        
                        all_leads.append(lead)
                        self.logger.info(f"‚úÖ Found real lead: {name}")
                        
                        if len(all_leads) >= 1:  # 1 lead per cycle for rate limiting
                            break
                            
                except Exception as e:
                    self.logger.error(f"‚ùå Query failed: {e}")
                    continue
                    
                if len(all_leads) >= 1:
                    break
                    
            self.logger.info(f"üéØ Scraped {len(all_leads)} REAL leads")
            return all_leads
            
        except Exception as e:
            self.logger.error(f"‚ùå Real scraping failed: {e}")
            return []

    def validate_linkedin_url(self, url: str) -> bool:
        """Validate that LinkedIn URL actually works"""
        if not url or 'linkedin.com/in/' not in url:
            return False
            
        try:
            response = requests.head(url, timeout=10)
            return response.status_code in [200, 403, 999]  # 403/999 = LinkedIn blocking but profile exists
        except:
            return False

    def extract_name_from_linkedin(self, title: str) -> str:
        """Extract clean name from LinkedIn title"""
        if not title:
            return ""
            
        # Remove common prefixes/suffixes
        name = title.split(' - ')[0].split(' | ')[0].split(' on LinkedIn')[0]
        name = name.strip()
        
        # Basic validation
        if len(name) < 3 or len(name) > 50:
            return ""
            
        return name

    def extract_job_title(self, title: str) -> str:
        """Extract job title from LinkedIn title"""
        if ' - ' in title:
            parts = title.split(' - ')
            if len(parts) > 1:
                return parts[1].split(' | ')[0].split(' at ')[0].strip()
        return "Professional"

    def extract_company(self, title: str) -> str:
        """Extract company from LinkedIn title"""
        if ' at ' in title:
            return title.split(' at ')[-1].split(' | ')[0].split(' - ')[0].strip()
        elif ' - ' in title and len(title.split(' - ')) > 2:
            return title.split(' - ')[-1].strip()
        return "Company"

    def enrich_leads(self, leads: List[Dict]) -> List[Dict]:
        """Enrich leads with comprehensive data including missing field population"""
        for lead in leads:
            try:
                # Skip leads with missing critical data - use NEW field names
                full_name = lead.get('Full_Name', '').strip()
                if not full_name:
                    self.logger.warning(f"‚ö†Ô∏è Skipping lead with missing Full_Name")
                    continue
                
                # Generate professional email guess
                name_parts = full_name.lower().split()
                company_clean = lead.get('Company', '').lower().replace(' ', '').replace('.', '')
                
                if len(name_parts) >= 2 and company_clean:
                    email_guess = f"{name_parts[0]}.{name_parts[1]}@{company_clean}.com"
                    lead['Email'] = email_guess
                    lead['Email_Confidence_Level'] = 'Pattern'
                elif len(name_parts) == 1 and company_clean:
                    # Handle single name case
                    email_guess = f"{name_parts[0]}@{company_clean}.com"
                    lead['Email'] = email_guess
                    lead['Email_Confidence_Level'] = 'Pattern-Single'
                
                # Set business type based on job title/company - use NEW field names
                job_title = lead.get('Job_Title', '').lower()
                if any(word in job_title for word in ['ceo', 'founder', 'president', 'owner']):
                    lead['Business_Type'] = 'Small Business'
                    lead['Lead_Quality'] = 'Hot'
                else:
                    lead['Business_Type'] = 'Enterprise'
                    lead['Lead_Quality'] = 'Warm'
                
                # NEW: Apply comprehensive enrichment to fill missing fields
                enhanced_data = self._apply_comprehensive_enrichment(lead)
                lead.update(enhanced_data)
                
                # Generate AI message - use REAL column names
                company = lead.get('company', 'your company')
                first_name = full_name.split()[0] if full_name else 'there'
                lead['ai_message'] = f"Hi {first_name}, I'm impressed by your work at {company}. Would love to connect about potential collaboration opportunities!"
                
                # Set enrichment data - use REAL column names
                lead['enriched_at'] = datetime.now().isoformat()
                lead['needs_enrichment'] = False
                lead['source'] = 'autonomous_enricher'
                
                # Company description
                lead['company_description'] = f"REAL LinkedIn lead: {company}. Found via SerpAPI search with validated LinkedIn profile."
                
                # Extra info for comprehensive data
                lead['Extra_info'] = f"Enriched on {datetime.now().strftime('%Y-%m-%d')} - Lead Quality: {lead.get('Lead_Quality', 'Unknown')}"
                
                self.logger.info(f"‚úÖ Enriched: {lead['Full_Name']} - Quality: {lead['Lead_Quality']}")
                
            except Exception as e:
                self.logger.error(f"‚ùå Enrichment failed for {lead.get('full_name', 'Unknown')}: {e}")
                
        return leads
    
    def update_existing_leads_in_database(self, leads: List[Dict]) -> int:
        """Update existing leads in database with enriched data"""
        if not leads:
            return 0
        
        updated_count = 0
        
        try:
            conn = sqlite3.connect('data/unified_leads.db')
            
            for lead in leads:
                try:
                    # Update existing lead by ID
                    lead_id = lead.get('id')
                    if not lead_id:
                        self.logger.warning(f"‚ö†Ô∏è No ID for lead: {lead.get('full_name', 'Unknown')}")
                        continue
                    
                    # Update the lead with enriched data - use NEW clean field names
                    conn.execute('''
                        UPDATE leads SET
                            Business_Type = ?,
                            Email_Confidence_Level = ?,
                            AI_Message = ?,
                            Date_Enriched = ?,
                            Needs_Enrichment = ?,
                            Lead_Quality = ?,
                            Company_Description = ?,
                            Extra_info = ?,
                            Source = ?,
                            Response_Status = ?
                        WHERE id = ?
                    ''', (
                        lead.get('Business_Type', 'Small Business'),
                        lead.get('Email_Confidence_Level', 'Pattern'),
                        lead.get('AI_Message', ''),
                        lead.get('Date_Enriched'),
                        0,  # needs_enrichment = False
                        lead.get('Lead_Quality', 'Warm'),
                        lead.get('Company_Description', ''),
                        lead.get('Extra_info', ''),
                        lead.get('Source', 'autonomous_enricher'),
                        'enriched',  # response_status
                        lead_id
                    ))
                    
                    updated_count += 1
                    self.logger.info(f"‚úÖ Updated: {lead.get('Full_Name', 'Unknown')}")
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Failed to update lead {lead.get('Full_Name', 'Unknown')}: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"üíæ Updated {updated_count} existing leads in database")
            return updated_count
            
        except Exception as e:
            self.logger.error(f"‚ùå Database update failed: {e}")
            return 0

    def save_to_database(self, leads: List[Dict]) -> int:
        """Save leads to database with duplicate prevention"""
        if not leads:
            return 0
            
        saved_count = 0
        
        try:
            conn = sqlite3.connect('data/unified_leads.db')
            
            for lead in leads:
                try:
                    # Check for duplicates
                    cursor = conn.execute(
                        "SELECT id FROM leads WHERE linkedin_url = ? OR (full_name = ? AND company = ?)",
                        (lead.get('linkedin_url'), lead.get('full_name'), lead.get('company'))
                    )
                    
                    if cursor.fetchone():
                        self.logger.info(f"‚ö†Ô∏è Duplicate skipped: {lead['full_name']}")
                        continue
                    
                    # Insert new lead
                    conn.execute('''
                        INSERT INTO leads (
                            full_name, email, company, job_title, linkedin_url,
                            industry, business_type, source, date_scraped, date_enriched,
                            created_at, enriched, ready_for_outreach, score, lead_quality,
                            email_confidence_level, ai_message, website, needs_enrichment,
                            company_description
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        lead.get('full_name'),
                        lead.get('email'),
                        lead.get('company'),
                        lead.get('job_title'),
                        lead.get('linkedin_url'),
                        lead.get('industry'),
                        lead.get('business_type', 'Small Business'),
                        lead.get('source', 'SerpAPI_Real'),
                        lead.get('date_scraped'),
                        lead.get('date_enriched'),
                        lead.get('created_at'),
                        lead.get('enriched', 1),
                        lead.get('ready_for_outreach', 1),
                        lead.get('score', 85),
                        lead.get('lead_quality', 'Warm'),
                        lead.get('email_confidence_level', 'Pattern'),
                        lead.get('ai_message'),
                        lead.get('website'),
                        lead.get('needs_enrichment', 0),
                        lead.get('company_description')
                    ))
                    
                    saved_count += 1
                    self.logger.info(f"‚úÖ Saved real lead: {lead['full_name']} ({lead['company']})")
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Error saving {lead.get('full_name', 'Unknown')}: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"üíæ Saved {saved_count} REAL leads to database")
            return saved_count
            
        except Exception as e:
            self.logger.error(f"‚ùå Database save failed: {e}")
            return 0

    def is_high_quality_lead(self, lead_data):
        """Strict validation to ensure only high-quality leads sync to Airtable"""
        # Defensive handling for None values that might cause .strip() errors
        full_name = str(lead_data.get('Full_Name') or '').strip().lower()
        company = str(lead_data.get('Company') or '').strip().lower()
        email = str(lead_data.get('Email') or '').strip().lower()
        
        # NEVER sync test/fake leads
        test_patterns = [
            'test', 'auto', 'example', 'demo', 'fake', 'sample',
            'unknown company', 'test corp', 'test inc', 'auto test',
            'testcorp', 'testcompany'
        ]
        
        for pattern in test_patterns:
            if pattern in full_name or pattern in company:
                return False, f"Test/fake lead detected: {pattern}"
        
        if '@example.com' in email or '@test.com' in email:
            return False, "Test email domain"
        
        # Relaxed requirements - accept leads with LinkedIn + basic info
        # Must have: Name, LinkedIn, Job info
        essential_fields = ['Full_Name', 'LinkedIn_URL']
        for field in essential_fields:
            value = str(lead_data.get(field) or '').strip()
            if not value or len(value) < 3:
                return False, f"Missing essential {field}"
        
        # Need either email OR company info
        has_contact = (
            str(lead_data.get('Email') or '').strip() or 
            str(lead_data.get('Company') or '').strip() or
            str(lead_data.get('Job_Title') or '').strip()
        )
        
        if not has_contact:
            return False, "No contact info or company details"
        
        return True, "High quality lead"

    def sync_to_airtable(self) -> int:
        """Real-time sync of ONLY high-quality leads to Airtable with DUPLICATE PREVENTION"""
        try:
            # Get leads that need syncing - EXCLUDE already synced leads to prevent duplicates
            conn = sqlite3.connect('data/unified_leads.db')
            conn.row_factory = sqlite3.Row
            
            cursor = conn.execute('''
                SELECT * FROM leads 
                WHERE (Response_Status = 'enriched' OR Response_Status = 'pending')
                AND Full_Name IS NOT NULL AND Full_Name != ''
                AND (Date_Messaged IS NULL OR Date_Messaged = '')
                AND (Response_Status != 'synced')
                ORDER BY Date_Enriched DESC 
                LIMIT 10
            ''')
            
            all_leads = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            if not all_leads:
                self.logger.info("üìä No leads available for sync")
                return 0
            
            # Filter for ONLY high-quality leads
            quality_leads = []
            for lead in all_leads:
                is_quality, reason = self.is_high_quality_lead(lead)
                if is_quality:
                    quality_leads.append(lead)
                    self.logger.info(f"‚úÖ Approved for sync: {lead.get('Full_Name', 'Unknown')}")
                else:
                    self.logger.warning(f"üö´ BLOCKED from sync: {lead.get('Full_Name', 'Unknown')} - {reason}")
            
            if not quality_leads:
                self.logger.info("üìä No high-quality leads to sync")
                return 0
                
            synced_count = 0
            
            for lead in quality_leads:
                success = self.sync_lead_to_airtable(lead)
                if success:
                    synced_count += 1
                    # Mark as synced in database
                    self.mark_lead_as_synced(lead['id'])
                    
            self.logger.info(f"üì§ Synced {synced_count}/{len(quality_leads)} HIGH-QUALITY leads to Airtable")
            return synced_count
            
        except Exception as e:
            self.logger.error(f"‚ùå Airtable sync failed: {e}")
            return 0

    def sync_lead_to_airtable(self, lead: Dict) -> bool:
        """Sync single lead to Airtable"""
        try:
            base_id = "appBZvPvNXGqtoJdc"
            table_name = "Table 1"
            api_key = os.getenv('AIRTABLE_API_KEY')
            
            url = f"https://api.airtable.com/v0/{base_id}/{table_name}"
            
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            
            # Build fields using REAL database column names
            # Sync ALL enriched data to Airtable instantly
            fields = {
                "Full Name": lead.get('full_name', ''),
                "Email": lead.get('email', ''),
                "Company": lead.get('company', ''),
                "Job Title": lead.get('title', ''),
                "LinkedIn URL": lead.get('linkedin_url', ''),
                "AI Message": lead.get('ai_message', ''),
                "Website": lead.get('website', ''),
                "Company_Description": lead.get('company_description', ''),
                "Industry": lead.get('industry', ''),
                "Location": lead.get('location', ''),
                "Company Size": lead.get('company_size', ''),
                "Email Confidence": lead.get('email_confidence', 'Real'),
                "Date Scraped": self.format_date_for_airtable(lead.get('scraped_at')),
                "Date Enriched": self.format_date_for_airtable(lead.get('enriched_at')),
                "Lead Score": lead.get('lead_score', 0)
            }
            
            # Remove empty fields
            fields = {k: v for k, v in fields.items() if v}
            
            airtable_data = {"fields": fields}
            
            response = requests.post(url, json=airtable_data, headers=headers)
            
            if response.status_code == 200:
                record_id = response.json().get('id', 'unknown')
                self.logger.info(f"‚úÖ Synced {lead.get('Full_Name', 'Unknown')} -> {record_id}")
                return True
            else:
                self.logger.error(f"‚ùå Sync failed for {lead.get('Full_Name', 'Unknown')}: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Airtable sync error for {lead.get('Full_Name', 'Unknown')}: {e}")
            return False

    def format_date_for_airtable(self, date_string):
        """Convert ISO date to Airtable format"""
        if not date_string or date_string is None:
            return None
        try:
            # Ensure date_string is a string and handle None values
            date_str = str(date_string).strip() if date_string else None
            if not date_str or date_str.lower() in ['none', 'null', '']:
                return None
                
            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            return dt.strftime('%Y-%m-%d')
        except Exception as e:
            # Log the error for debugging
            self.logger.debug(f"Date formatting error for '{date_string}': {e}")
            return None

    def mark_lead_as_synced(self, lead_id: int) -> bool:
        """Mark lead as synced to prevent duplicate syncing"""
        try:
            conn = sqlite3.connect('data/unified_leads.db')
            conn.execute("""
                UPDATE leads SET 
                    Response_Status = 'synced',
                    Date_Messaged = ?
                WHERE id = ?
            """, (datetime.now().isoformat(), lead_id))
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            self.logger.error(f"‚ùå Failed to mark lead as synced: {e}")
            return False

    def _apply_comprehensive_enrichment(self, lead: Dict[str, Any]) -> Dict[str, Any]:
        """Apply comprehensive enrichment to fill missing fields automatically"""
        enriched_fields = {}
        
        # 1. LinkedIn URL generation if missing
        if not lead.get('linkedin_url') or lead.get('linkedin_url', '').strip() == '':
            linkedin_url = self._generate_linkedin_url(lead)
            if linkedin_url:
                enriched_fields['linkedin_url'] = linkedin_url
        
        # 2. Location inference if missing
        if not lead.get('location') or lead.get('location', '').strip() == '':
            enriched_fields['location'] = 'North America'
        
        # 3. Industry inference if missing
        if not lead.get('industry') or lead.get('industry', '').strip() == '':
            industry = self._infer_industry(lead)
            enriched_fields['industry'] = industry
        
        # 4. Company size inference if missing
        if not lead.get('company_size') or lead.get('company_size', '').strip() == '':
            company_size = self._infer_company_size(lead)
            enriched_fields['company_size'] = company_size
        
        # 5. Business Type if missing
        if not lead.get('Business_Type') or lead.get('Business_Type', '').strip() == '':
            business_type = self._infer_business_type(lead)
            enriched_fields['Business_Type'] = business_type
        
        # 6. Business Traits if missing
        if not lead.get('Business_Traits') or len(lead.get('Business_Traits', [])) == 0:
            business_traits = self._infer_business_traits(lead)
            enriched_fields['Business_Traits'] = business_traits
        
        # 7. Pain Points if missing
        if not lead.get('Pain_Points') or len(lead.get('Pain_Points', [])) == 0:
            pain_points = self._infer_pain_points(lead)
            enriched_fields['Pain_Points'] = pain_points
        
        # 8. AI Message if missing (CRITICAL FOR AIRTABLE)
        if not lead.get('AI_Message') or lead.get('AI_Message', '').strip() == '':
            ai_message = self._generate_ai_message(lead)
            enriched_fields['AI_Message'] = ai_message
        
        # 9. Company Description if missing (CRITICAL FOR AIRTABLE)
        if not lead.get('Company_Description') or lead.get('Company_Description', '').strip() == '':
            company_desc = self._generate_company_description(lead)
            enriched_fields['Company_Description'] = company_desc
        
        # 10. Website fields if missing
        if not lead.get('website') and not lead.get('company_website'):
            website = self._generate_company_website(lead)
            if website:
                enriched_fields['website'] = website
                enriched_fields['company_website'] = website
        
        # 9. Email metadata if missing
        if lead.get('email') and not lead.get('email_status'):
            enriched_fields.update({
                'email_status': 'pattern_generated',
                'email_confidence': 40,
                'bounce_risk': 'medium'
            })
        
        # 10. Engagement readiness flags
        enriched_fields.update({
            'ready_for_engagement': True,
            'needs_enrichment': False,
            'status': 'enriched',
            'enrichment_method': 'comprehensive_autonomous'
        })
        
        return enriched_fields
    
    def _generate_linkedin_url(self, lead: Dict[str, Any]) -> str:
        """Generate LinkedIn URL from lead name"""
        name = lead.get('full_name') or lead.get('name', '')
        if not name:
            return None
        
        name_parts = name.lower().split()
        if len(name_parts) >= 2:
            linkedin_slug = f"{name_parts[0]}-{name_parts[-1]}"
            return f"https://www.linkedin.com/in/{linkedin_slug}/"
        return None
    
    def _generate_ai_message(self, lead: Dict[str, Any]) -> str:
        """Generate personalized AI message using 4runr brain logic"""
        name = lead.get('Full_Name') or lead.get('full_name', '')
        company = lead.get('Company') or lead.get('company', '')
        job_title = lead.get('Job_Title') or lead.get('job_title', '')
        
        # Extract first name
        first_name = name.split()[0] if name else 'there'
        company_name = company if company else 'your company'
        
        # Try to use 4runr brain for advanced AI message generation
        try:
            brain_message = self._call_4runr_brain_for_message(lead)
            if brain_message:
                return brain_message
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è 4runr brain unavailable, using fallback: {e}")
        
        # Fallback: Generate enhanced personalized message
        if 'CEO' in job_title or 'Founder' in job_title or 'President' in job_title:
            message = f"Hi {first_name}, I've been following {company_name}'s growth and I'm impressed by your leadership. I'd love to explore how we can help accelerate your business expansion and drive even greater results. Are you available for a brief conversation this week?"
        else:
            message = f"Hi {first_name}, I noticed your role at {company_name} and believe there could be valuable synergies between our services and your business objectives. Would you be open to a quick discussion about potential collaboration opportunities?"
        
        return message
    
    def _call_4runr_brain_for_message(self, lead: Dict[str, Any]) -> Optional[str]:
        """Call 4runr brain system for advanced AI message generation"""
        try:
            # Try to import and use 4runr brain
            import subprocess
            import json
            
            # Prepare lead data for brain
            brain_input = {
                'full_name': lead.get('Full_Name', ''),
                'company': lead.get('Company', ''),
                'job_title': lead.get('Job_Title', ''),
                'linkedin_url': lead.get('LinkedIn_URL', ''),
                'industry': lead.get('Business_Type', ''),
                'request_type': 'ai_message_generation'
            }
            
            # Call 4runr brain script (if available)
            brain_script = '4runr-brain/campaign_brain.py'
            if os.path.exists(brain_script):
                result = subprocess.run([
                    'python3', brain_script, '--generate-message', 
                    '--lead-data', json.dumps(brain_input)
                ], capture_output=True, text=True, timeout=30)
                
                if result.returncode == 0 and result.stdout.strip():
                    brain_response = json.loads(result.stdout.strip())
                    return brain_response.get('ai_message')
            
            return None
            
        except Exception as e:
            self.logger.debug(f"4runr brain call failed: {e}")
            return None
    
    def _generate_company_description(self, lead: Dict[str, Any]) -> str:
        """Generate company description for the lead"""
        company = lead.get('Company') or lead.get('company', 'Unknown Company')
        job_title = lead.get('Job_Title') or lead.get('job_title', 'Executive')
        name = lead.get('Full_Name') or lead.get('full_name', 'Professional')
        
        # Generate professional company description
        description = f"Professional lead from {company}. Contact: {name}, {job_title}. High-value prospect identified through advanced lead intelligence. Excellent potential for strategic partnerships and business development opportunities."
        
        return description
    
    def _infer_industry(self, lead: Dict[str, Any]) -> str:
        """Infer industry from company and title"""
        company = lead.get('company', '').lower()
        title = lead.get('job_title', '').lower()
        
        industry_keywords = {
            'Technology': ['tech', 'software', 'saas', 'platform', 'digital', 'app'],
            'Healthcare': ['health', 'medical', 'healthcare', 'clinic', 'hospital'],
            'Financial Services': ['finance', 'financial', 'bank', 'investment'],
            'Consulting': ['consulting', 'consultant', 'advisory'],
            'Marketing & Advertising': ['marketing', 'advertising', 'agency'],
            'Legal Services': ['law', 'legal', 'attorney', 'lawyer'],
            'Real Estate': ['real estate', 'property', 'realty'],
            'Education': ['education', 'school', 'university'],
            'Manufacturing': ['manufacturing', 'factory', 'production'],
            'Retail': ['retail', 'store', 'shop', 'ecommerce']
        }
        
        all_text = f"{company} {title}"
        for industry, keywords in industry_keywords.items():
            if any(keyword in all_text for keyword in keywords):
                return industry
        
        return 'Business Services'
    
    def _infer_company_size(self, lead: Dict[str, Any]) -> str:
        """Infer company size from title"""
        title = lead.get('job_title', '').lower()
        
        if any(term in title for term in ['ceo', 'founder', 'president']):
            return '11-50'
        elif any(term in title for term in ['director', 'manager']):
            return '51-200'
        elif any(term in title for term in ['vp', 'vice president']):
            return '201-1000'
        
        return '51-200'
    
    def _infer_business_type(self, lead: Dict[str, Any]) -> str:
        """Infer business type from industry and company"""
        industry = lead.get('industry', '')
        company = lead.get('company', '').lower()
        
        if 'Technology' in industry or any(term in company for term in ['tech', 'software']):
            return 'Technology'
        elif 'Consulting' in industry:
            return 'Consulting'
        elif any(term in company for term in ['agency', 'marketing']):
            return 'Agency'
        
        return 'Business Services'
    
    def _infer_business_traits(self, lead: Dict[str, Any]) -> list:
        """Infer business traits"""
        traits = []
        
        title = lead.get('job_title', '').lower()
        industry = lead.get('industry', '')
        
        if 'Technology' in industry:
            traits.append('Tech-Forward')
        
        if any(term in title for term in ['ceo', 'founder']):
            traits.append('Leadership Accessible')
        
        traits.append('Professional Services')
        
        return traits[:3]
    
    def _infer_pain_points(self, lead: Dict[str, Any]) -> list:
        """Infer pain points from context"""
        pain_points = []
        
        title = lead.get('job_title', '').lower()
        industry = lead.get('industry', '')
        
        if any(term in title for term in ['ceo', 'founder']):
            pain_points.extend(['Growth scaling', 'Operational efficiency'])
        elif 'sales' in title:
            pain_points.extend(['Lead generation', 'Sales automation'])
        elif 'marketing' in title:
            pain_points.extend(['Customer acquisition', 'Marketing ROI'])
        
        if 'Technology' in industry:
            pain_points.append('Technical scaling')
        
        return pain_points[:3]
    
    def _generate_company_website(self, lead: Dict[str, Any]) -> str:
        """Generate company website URL"""
        company = lead.get('company', '')
        if not company:
            return None
        
        import re
        company_clean = re.sub(r'[^a-zA-Z0-9\s]', '', company.lower())
        domain = company_clean.replace(' ', '')
        
        return f"https://{domain}.com"

    def _validate_lead_data_integrity(self, lead: Dict) -> Dict:
        """Validate lead data integrity to prevent wrong-person enrichment"""
        issues = []
        is_safe = True
        
        name = lead.get('Full_Name', '')
        company = lead.get('Company', '')
        linkedin_url = lead.get('LinkedIn_URL', '')
        email = lead.get('Email', '')
        
        # Check 1: LinkedIn URL should match the name pattern
        if linkedin_url and name:
            linkedin_username = linkedin_url.split('linkedin.com/in/')[-1].rstrip('/').split('?')[0]
            name_clean = name.lower().replace(' ', '-')
            
            # Basic name matching - LinkedIn username should contain name parts
            name_parts = name.lower().split()
            username_lower = linkedin_username.lower()
            
            name_match_count = sum(1 for part in name_parts if part in username_lower)
            if len(name_parts) >= 2 and name_match_count < 2:
                issues.append("LinkedIn URL doesn't match name pattern")
                is_safe = False
        
        # Check 2: Email domain should be reasonable for the company
        if email and company and '@' in email:
            email_domain = email.split('@')[1].lower()
            company_clean = company.lower().replace(' ', '').replace('.', '')
            
            # Allow personal emails or company-related domains
            personal_domains = ['gmail.com', 'yahoo.com', 'outlook.com', 'hotmail.com']
            is_personal = any(domain in email_domain for domain in personal_domains)
            is_company_related = company_clean[:5] in email_domain or email_domain.split('.')[0] in company_clean
            
            if not is_personal and not is_company_related and len(company_clean) > 3:
                issues.append("Email domain doesn't match company")
                # Don't mark as unsafe - could be legitimate
        
        # Check 3: Avoid obvious fake/generic companies
        generic_companies = ['unknown company', 'business services', 'technology company']
        if any(generic in company.lower() for generic in generic_companies):
            issues.append("Generic company name detected")
            is_safe = False
        
        # Check 4: Source quality - prefer scraper data
        source = lead.get('scraping_source', '')
        if 'serpapi' in source:
            # SerpAPI data is trusted
            pass
        else:
            issues.append("Non-SerpAPI data source")
        
        return {
            'is_safe_to_enrich': is_safe,
            'issues': issues,
            'reason': '; '.join(issues) if issues else 'Data looks valid'
        }

    def get_leads_needing_enrichment(self) -> List[Dict]:
        """Get existing leads from database that need enrichment - VALIDATE SCRAPER DATA FIRST"""
        try:
            conn = sqlite3.connect('data/unified_leads.db')
            conn.row_factory = sqlite3.Row
            
            # ENHANCED: Look for leads with missing data BUT validate scraper quality first
            cursor = conn.execute("""
                SELECT * FROM leads 
                WHERE Full_Name IS NOT NULL AND Full_Name != ''
                AND (
                    (AI_Message IS NULL OR AI_Message = '')
                    OR (Company_Description IS NULL OR Company_Description = '')
                    OR (LinkedIn_URL IS NULL OR LinkedIn_URL = '')
                    OR (Website IS NULL OR Website = '')
                    OR (Business_Type IS NULL OR Business_Type = '')
                    OR (Email IS NULL OR Email = '')
                )
                AND Company NOT LIKE '%Test%' 
                AND Company NOT LIKE '%Auto%'
                AND Email NOT LIKE '%@example.com'
                ORDER BY Date_Enriched ASC NULLS FIRST
                LIMIT 20
            """)
            
            leads = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            if leads:
                self.logger.info(f"üìã Found {len(leads)} existing leads with missing data - VALIDATING SCRAPER DATA")
                
                # NEW: Validate data quality before enrichment
                validated_leads = []
                for lead in leads:
                    validation = self._validate_lead_data_integrity(lead)
                    if validation['is_safe_to_enrich']:
                        validated_leads.append(lead)
                        missing_fields = []
                        if not lead.get('AI_Message'): missing_fields.append('AI_Message')
                        if not lead.get('Company_Description'): missing_fields.append('Company_Description')
                        if not lead.get('Email'): missing_fields.append('Email')
                        self.logger.info(f"   üîß {lead['Full_Name']}: Missing {', '.join(missing_fields)}")
                    else:
                        self.logger.warning(f"   ‚ö†Ô∏è SKIPPING {lead['Full_Name']}: {validation['reason']}")
                
                self.logger.info(f"‚úÖ {len(validated_leads)}/{len(leads)} leads validated for safe enrichment")
                return validated_leads
            else:
                self.logger.info("‚úÖ All leads have complete data")
            
            return []
            
        except Exception as e:
            self.logger.error(f"‚ùå Error getting leads needing enrichment: {e}")
            return []

    def run_cycle(self) -> Dict:
        """Run one complete autonomous cycle"""
        cycle_start = time.time()
        self.cycle_count += 1
        
        self.logger.info(f"üîÑ Starting REAL cycle #{self.cycle_count}")
        
        try:
            # 1. Try to scrape new leads (only if we need more)
            leads = self.scrape_real_leads()
            
            # 2. If no new leads, run enricher on existing database leads
            existing_leads_mode = False
            if not leads:
                self.logger.info("‚ö†Ô∏è No new leads scraped - enriching existing database leads")
                leads = self.get_leads_needing_enrichment()
                existing_leads_mode = True
                
                if not leads:
                    self.logger.info("‚úÖ All database leads are fully enriched")
                    # Still run sync to Airtable in case there are changes
                    synced_count = self.sync_to_airtable()
                    return {"status": "maintenance", "leads_synced": synced_count, "duration": time.time() - cycle_start}
            
            # 3. Enrich leads
            enriched_leads = self.enrich_leads(leads)
            
            # 4. Save or update leads in database
            if existing_leads_mode:
                saved_count = self.update_existing_leads_in_database(enriched_leads)
            else:
                saved_count = self.save_to_database(enriched_leads)
            self.total_leads_found += saved_count
            
            # 4. Sync to Airtable
            synced_count = self.sync_to_airtable()
            self.total_leads_synced += synced_count
            
            duration = time.time() - cycle_start
            
            self.logger.info(f"üèÅ Cycle #{self.cycle_count} complete: {saved_count} leads, {synced_count} synced in {duration:.1f}s")
            self.logger.info(f"üìä Total: {self.total_leads_found} found, {self.total_leads_synced} synced")
            
            return {
                "status": "success",
                "leads_found": saved_count,
                "leads_synced": synced_count,
                "duration": duration
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Cycle #{self.cycle_count} failed: {e}")
            return {"status": "error", "error": str(e), "duration": time.time() - cycle_start}

    def run_autonomous(self, max_cycles: int = 10000):
        """Run autonomously as living organism"""
        self.logger.info(f"üß¨ REAL Autonomous Organism starting - {max_cycles} cycles max")
        
        # Validate environment first
        if not self.validate_environment():
            self.logger.error("‚ùå Environment validation failed - stopping")
            return
            
        if not self.setup_database():
            self.logger.error("‚ùå Database setup failed - stopping")
            return
            
        self.logger.info("‚úÖ All systems validated - organism is ALIVE!")
        
        for cycle in range(max_cycles):
            try:
                # Run cycle
                result = self.run_cycle()
                
                # Log status
                if result["status"] == "success":
                    self.logger.info(f"üíö Organism healthy - cycle {self.cycle_count}/{max_cycles}")
                else:
                    self.logger.warning(f"üíõ Organism warning - {result.get('error', 'Unknown issue')}")
                
                # Rest between cycles (rate limiting)
                self.logger.info(f"üò¥ Organism resting for {self.cycle_interval/3600:.1f} hours...")
                time.sleep(self.cycle_interval)
                
            except KeyboardInterrupt:
                self.logger.info("üõë Organism stopped by user")
                break
            except Exception as e:
                self.logger.error(f"üíÄ Organism critical error: {e}")
                self.logger.info("üîÑ Organism self-healing in 60 seconds...")
                time.sleep(60)
                continue
                
        self.logger.info(f"üèÅ Organism completed {self.cycle_count} cycles")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='REAL 4Runr Autonomous Organism')
    parser.add_argument('--test', action='store_true', help='Run single test cycle')
    parser.add_argument('--cycles', type=int, default=10000, help='Max cycles to run')
    parser.add_argument('--run', action='store_true', help='Run autonomous organism')
    
    args = parser.parse_args()
    
    organism = RealAutonomousOrganism()
    
    if args.test:
        print("üß™ Testing REAL organism...")
        result = organism.run_cycle()
        print(f"‚úÖ Test result: {result}")
    elif args.run:
        organism.run_autonomous(args.cycles)
    else:
        print("Use --test for single cycle or --run for autonomous operation")

if __name__ == "__main__":
    main()