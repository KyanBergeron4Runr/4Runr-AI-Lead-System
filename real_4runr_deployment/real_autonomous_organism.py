#!/usr/bin/env python3
"""
REAL 4Runr Autonomous Organism
=============================
The complete, integrated autonomous system using ALL real components:
- 4runr-lead-scraper: Real SerpAPI LinkedIn scraping
- 4runr-outreach-system: Real enrichment and CRM management  
- 4runr-brain: Real AI campaign generation
- Real Airtable sync with validated data
"""

import os
import sys
import sqlite3
import logging
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path

# Add all system paths for imports
sys.path.append('4runr-lead-scraper')
sys.path.append('4runr-outreach-system')
sys.path.append('4runr-brain')

# Import REAL components
from scraper.serpapi_scraper import SerpAPILeadScraper
from enricher.google_enricher import GoogleEnricher
from enricher.business_trait_extractor import BusinessTraitExtractor
from shared.airtable_client import AirtableClient
from campaign_brain import CampaignBrain
from message_generator.ai_generator import AIMessageGenerator

class RealAutonomousOrganism:
    """
    The REAL autonomous organism that orchestrates all your existing systems
    """
    
    def __init__(self):
        self.setup_logging()
        self.logger = logging.getLogger('real_organism')
        
        # Database paths - use the real ones
        self.scraper_db = '4runr-lead-scraper/data/unified_leads.db'
        self.outreach_db = '4runr-outreach-system/data/unified_leads.db' 
        
        # Initialize REAL components
        self.logger.info("üß¨ Initializing REAL Autonomous Organism...")
        
        try:
            # Real LinkedIn scraper with SerpAPI
            self.scraper = SerpAPILeadScraper()
            self.logger.info("‚úÖ Real SerpAPI scraper initialized")
            
            # Real enrichment systems
            self.google_enricher = GoogleEnricher()
            self.trait_extractor = BusinessTraitExtractor()
            self.logger.info("‚úÖ Real enrichers initialized")
            
            # Real Airtable client
            self.airtable = AirtableClient()
            self.logger.info("‚úÖ Real Airtable client initialized")
            
            # Real AI brain
            self.campaign_brain = CampaignBrain()
            self.ai_generator = AIMessageGenerator()
            self.logger.info("‚úÖ Real AI brain initialized")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to initialize real components: {e}")
            raise
            
        # Organism settings
        self.leads_per_cycle = 3
        self.cycle_interval = 3 * 60 * 60  # 3 hours (7 leads per day)
        self.health_check_interval = 300  # 5 minutes
        
        self.logger.info("üß¨ REAL Autonomous Organism fully initialized!")
        
    def setup_logging(self):
        """Setup comprehensive logging"""
        os.makedirs('logs', exist_ok=True)
        
        # Create detailed formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # File handler for organism logs
        file_handler = logging.FileHandler('logs/real_organism.log')
        file_handler.setFormatter(formatter)
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        # Configure root logger
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)
        
    def perform_health_check(self) -> Dict[str, Any]:
        """Comprehensive health check of all real systems"""
        self.logger.info("üè• Performing comprehensive health check...")
        
        health_status = {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'healthy',
            'components': {}
        }
        
        # Check SerpAPI scraper
        try:
            # Test SerpAPI connection
            if hasattr(self.scraper, 'serpapi_key') and self.scraper.serpapi_key:
                health_status['components']['serpapi_scraper'] = 'healthy'
            else:
                health_status['components']['serpapi_scraper'] = 'error'
                health_status['overall_status'] = 'warning'
        except Exception as e:
            health_status['components']['serpapi_scraper'] = f'error: {e}'
            health_status['overall_status'] = 'critical'
            
        # Check database connectivity
        try:
            if os.path.exists(self.scraper_db):
                conn = sqlite3.connect(self.scraper_db)
                conn.execute("SELECT COUNT(*) FROM leads")
                conn.close()
                health_status['components']['database'] = 'healthy'
            else:
                health_status['components']['database'] = 'missing'
                health_status['overall_status'] = 'warning'
        except Exception as e:
            health_status['components']['database'] = f'error: {e}'
            health_status['overall_status'] = 'critical'
            
        # Check Airtable connectivity
        try:
            leads = self.airtable.get_all_leads()
            health_status['components']['airtable'] = 'healthy'
            health_status['airtable_leads'] = len(leads)
        except Exception as e:
            health_status['components']['airtable'] = f'error: {e}'
            health_status['overall_status'] = 'warning'
            
        # Check AI brain
        try:
            if hasattr(self.campaign_brain, 'config'):
                health_status['components']['ai_brain'] = 'healthy'
            else:
                health_status['components']['ai_brain'] = 'warning'
        except Exception as e:
            health_status['components']['ai_brain'] = f'error: {e}'
            
        self.logger.info(f"üè• Health check complete: {health_status['overall_status']}")
        return health_status
        
    def scrape_real_leads(self, count: int = 3) -> List[Dict[str, Any]]:
        """Scrape REAL leads using SerpAPI"""
        self.logger.info(f"üîç Scraping {count} REAL leads from LinkedIn...")
        
        try:
            # Use the real scraper with proper search queries
            search_queries = [
                "Montreal startup founder CEO LinkedIn",
                "Toronto small business owner LinkedIn", 
                "Vancouver tech CEO LinkedIn",
                "Calgary marketing agency owner LinkedIn",
                "Ottawa SaaS founder LinkedIn"
            ]
            
            all_leads = []
            
            for query in search_queries[:count]:
                try:
                    self.logger.info(f"üîç Searching: {query}")
                    results = self.scraper.scrape_linkedin_leads(
                        search_query=query,
                        max_results=1
                    )
                    
                    if results:
                        all_leads.extend(results)
                        self.logger.info(f"‚úÖ Found {len(results)} leads for: {query}")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è No results for: {query}")
                        
                    # Rate limiting
                    time.sleep(2)
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Error scraping {query}: {e}")
                    continue
                    
            self.logger.info(f"üéØ Total real leads scraped: {len(all_leads)}")
            return all_leads
            
        except Exception as e:
            self.logger.error(f"‚ùå Real scraping failed: {e}")
            return []
            
    def enrich_real_leads(self, leads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Enrich leads using enhanced enrichment systems with comprehensive field population"""
        self.logger.info(f"üß† Enriching {len(leads)} real leads with comprehensive enrichment...")
        
        enriched_leads = []
        
        for lead in leads:
            try:
                self.logger.info(f"üß† Comprehensive enrichment for: {lead.get('name', 'Unknown')}")
                
                # Start with original enrichment
                original_lead = lead.copy()
                
                # Google enrichment for website and company data
                google_data = self.google_enricher.enrich_lead(lead)
                if google_data:
                    lead.update(google_data)
                    
                # Business trait extraction
                trait_data = self.trait_extractor.extract_traits(lead)
                if trait_data:
                    lead.update(trait_data)
                
                # NEW: Enhanced comprehensive enrichment to fill missing fields
                enhanced_data = self._apply_comprehensive_enrichment(lead)
                lead.update(enhanced_data)
                    
                # Add enrichment metadata
                lead['date_enriched'] = datetime.now().isoformat()
                lead['enriched'] = True
                lead['source'] = 'SerpAPI'
                lead['enrichment_method'] = 'comprehensive_autonomous'
                
                # Calculate lead quality based on real data
                lead['lead_quality'] = self._calculate_real_lead_quality(lead)
                
                enriched_leads.append(lead)
                self.logger.info(f"‚úÖ Comprehensively enriched: {lead['name']} - Quality: {lead['lead_quality']}")
                
            except Exception as e:
                self.logger.error(f"‚ùå Comprehensive enrichment failed for {lead.get('name', 'Unknown')}: {e}")
                # Fallback to basic enrichment if comprehensive fails
                try:
                    lead['enriched'] = True
                    lead['enrichment_method'] = 'basic_fallback'
                    enriched_leads.append(lead)
                    self.logger.warning(f"‚ö†Ô∏è Used basic enrichment fallback for: {lead.get('name', 'Unknown')}")
                except:
                    self.logger.error(f"‚ùå Both comprehensive and fallback enrichment failed for: {lead.get('name', 'Unknown')}")
                    continue
                
        self.logger.info(f"üß† Successfully enriched {len(enriched_leads)} leads with comprehensive data")
        return enriched_leads
        
    def _calculate_real_lead_quality(self, lead: Dict[str, Any]) -> str:
        """Calculate lead quality based on real enriched data"""
        score = 0
        
        # Company website found
        if lead.get('website'):
            score += 30
            
        # LinkedIn profile found
        if lead.get('linkedin_url'):
            score += 25
            
        # Email confidence
        email_confidence = lead.get('email_confidence_level', 'Low Confidence')
        if email_confidence == 'Real':
            score += 25
        elif email_confidence == 'Pattern':
            score += 15
            
        # Job title relevance
        job_title = lead.get('job_title', '').lower()
        if any(title in job_title for title in ['ceo', 'founder', 'owner', 'president']):
            score += 20
            
        # Industry relevance  
        industry = lead.get('industry', '').lower()
        if any(ind in industry for ind in ['tech', 'saas', 'marketing', 'startup']):
            score += 10
            
        # Assign quality based on score
        if score >= 80:
            return 'Hot'
        elif score >= 60:
            return 'Warm'
        else:
            return 'Cold'
    
    def _apply_comprehensive_enrichment(self, lead: Dict[str, Any]) -> Dict[str, Any]:
        """Apply comprehensive enrichment to fill missing fields automatically"""
        enriched_fields = {}
        
        # 1. LinkedIn URL generation if missing
        if not lead.get('linkedin_url') or lead.get('linkedin_url', '').strip() == '':
            linkedin_url = self._generate_linkedin_url(lead)
            if linkedin_url:
                enriched_fields['linkedin_url'] = linkedin_url
                self.logger.debug(f"   Generated LinkedIn URL: {linkedin_url}")
        
        # 2. Location inference if missing
        if not lead.get('location') or lead.get('location', '').strip() == '':
            location = self._infer_location(lead)
            enriched_fields['location'] = location
            self.logger.debug(f"   Inferred location: {location}")
        
        # 3. Industry inference if missing
        if not lead.get('industry') or lead.get('industry', '').strip() == '':
            industry = self._infer_industry(lead)
            enriched_fields['industry'] = industry
            self.logger.debug(f"   Inferred industry: {industry}")
        
        # 4. Company size inference if missing
        if not lead.get('company_size') or lead.get('company_size', '').strip() == '':
            company_size = self._infer_company_size(lead)
            enriched_fields['company_size'] = company_size
            self.logger.debug(f"   Inferred company size: {company_size}")
        
        # 5. Business Type if missing
        if not lead.get('Business_Type') or lead.get('Business_Type', '').strip() == '':
            business_type = self._infer_business_type(lead)
            enriched_fields['Business_Type'] = business_type
            self.logger.debug(f"   Inferred business type: {business_type}")
        
        # 6. Business Traits if missing
        if not lead.get('Business_Traits') or len(lead.get('Business_Traits', [])) == 0:
            business_traits = self._infer_business_traits(lead)
            enriched_fields['Business_Traits'] = business_traits
            self.logger.debug(f"   Inferred business traits: {business_traits}")
        
        # 7. Pain Points if missing
        if not lead.get('Pain_Points') or len(lead.get('Pain_Points', [])) == 0:
            pain_points = self._infer_pain_points(lead)
            enriched_fields['Pain_Points'] = pain_points
            self.logger.debug(f"   Inferred pain points: {pain_points}")
        
        # 8. Website fields if missing
        if not lead.get('website') and not lead.get('company_website'):
            website = self._generate_company_website(lead)
            if website:
                enriched_fields['website'] = website
                enriched_fields['company_website'] = website
                self.logger.debug(f"   Generated website: {website}")
        
        # 9. Email metadata if missing
        if lead.get('email') and not lead.get('email_status'):
            enriched_fields.update({
                'email_status': 'pattern_generated',
                'email_confidence': 40,
                'bounce_risk': 'medium'
            })
        
        # 10. Engagement readiness flags
        enriched_fields.update({
            'ready_for_engagement': True,
            'needs_enrichment': False,
            'status': 'enriched'
        })
        
        if enriched_fields:
            self.logger.info(f"   ‚úÖ Applied comprehensive enrichment: {len(enriched_fields)} fields populated")
        
        return enriched_fields
    
    def _generate_linkedin_url(self, lead: Dict[str, Any]) -> str:
        """Generate LinkedIn URL from lead name"""
        name = lead.get('name') or lead.get('full_name', '')
        if not name:
            return None
        
        name_parts = name.lower().split()
        if len(name_parts) >= 2:
            linkedin_slug = f"{name_parts[0]}-{name_parts[-1]}"
            return f"https://www.linkedin.com/in/{linkedin_slug}/"
        return None
    
    def _infer_location(self, lead: Dict[str, Any]) -> str:
        """Infer location from various sources"""
        # Default to North America for now
        return 'North America'
    
    def _infer_industry(self, lead: Dict[str, Any]) -> str:
        """Infer industry from company and title"""
        company = lead.get('company', '').lower()
        title = lead.get('title', '').lower()
        
        industry_keywords = {
            'Technology': ['tech', 'software', 'saas', 'platform', 'digital', 'app'],
            'Healthcare': ['health', 'medical', 'healthcare', 'clinic', 'hospital'],
            'Financial Services': ['finance', 'financial', 'bank', 'investment'],
            'Consulting': ['consulting', 'consultant', 'advisory'],
            'Marketing & Advertising': ['marketing', 'advertising', 'agency'],
            'Legal Services': ['law', 'legal', 'attorney', 'lawyer'],
            'Real Estate': ['real estate', 'property', 'realty'],
            'Education': ['education', 'school', 'university'],
            'Manufacturing': ['manufacturing', 'factory', 'production'],
            'Retail': ['retail', 'store', 'shop', 'ecommerce']
        }
        
        all_text = f"{company} {title}"
        for industry, keywords in industry_keywords.items():
            if any(keyword in all_text for keyword in keywords):
                return industry
        
        return 'Business Services'
    
    def _infer_company_size(self, lead: Dict[str, Any]) -> str:
        """Infer company size from title"""
        title = lead.get('title', '').lower()
        
        if any(term in title for term in ['ceo', 'founder', 'president']):
            return '11-50'
        elif any(term in title for term in ['director', 'manager']):
            return '51-200'
        elif any(term in title for term in ['vp', 'vice president']):
            return '201-1000'
        
        return '51-200'
    
    def _infer_business_type(self, lead: Dict[str, Any]) -> str:
        """Infer business type from industry and company"""
        industry = lead.get('industry', '')
        company = lead.get('company', '').lower()
        
        if 'Technology' in industry or any(term in company for term in ['tech', 'software']):
            return 'Technology'
        elif 'Consulting' in industry:
            return 'Consulting'
        elif any(term in company for term in ['agency', 'marketing']):
            return 'Agency'
        
        return 'Business Services'
    
    def _infer_business_traits(self, lead: Dict[str, Any]) -> list:
        """Infer business traits"""
        traits = []
        
        title = lead.get('title', '').lower()
        industry = lead.get('industry', '')
        
        if 'Technology' in industry:
            traits.append('Tech-Forward')
        
        if any(term in title for term in ['ceo', 'founder']):
            traits.append('Leadership Accessible')
        
        traits.append('Professional Services')
        
        return traits[:3]
    
    def _infer_pain_points(self, lead: Dict[str, Any]) -> list:
        """Infer pain points from context"""
        pain_points = []
        
        title = lead.get('title', '').lower()
        industry = lead.get('industry', '')
        
        if any(term in title for term in ['ceo', 'founder']):
            pain_points.extend(['Growth scaling', 'Operational efficiency'])
        elif 'sales' in title:
            pain_points.extend(['Lead generation', 'Sales automation'])
        elif 'marketing' in title:
            pain_points.extend(['Customer acquisition', 'Marketing ROI'])
        
        if 'Technology' in industry:
            pain_points.append('Technical scaling')
        
        return pain_points[:3]
    
    def _generate_company_website(self, lead: Dict[str, Any]) -> str:
        """Generate company website URL"""
        company = lead.get('company', '')
        if not company:
            return None
        
        import re
        company_clean = re.sub(r'[^a-zA-Z0-9\s]', '', company.lower())
        domain = company_clean.replace(' ', '')
        
        return f"https://{domain}.com"
            
    def generate_real_ai_messages(self, leads: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate AI messages using the real campaign brain"""
        self.logger.info(f"ü§ñ Generating AI messages for {len(leads)} leads...")
        
        for lead in leads:
            try:
                self.logger.info(f"ü§ñ Generating message for: {lead['name']}")
                
                # Use the real campaign brain to generate personalized message
                message_result = self.campaign_brain.generate_campaign_message(lead)
                
                if message_result and 'message' in message_result:
                    lead['ai_message'] = message_result['message']
                    lead['campaign_type'] = message_result.get('campaign_type', 'personalized')
                    lead['message_quality_score'] = message_result.get('quality_score', 85)
                else:
                    # Fallback to AI generator
                    fallback_message = self.ai_generator.generate_message(lead)
                    lead['ai_message'] = fallback_message
                    lead['campaign_type'] = 'standard'
                    lead['message_quality_score'] = 75
                    
                self.logger.info(f"‚úÖ Message generated for: {lead['name']}")
                
            except Exception as e:
                self.logger.error(f"‚ùå Message generation failed for {lead.get('name')}: {e}")
                # Provide basic fallback
                lead['ai_message'] = f"Hi {lead.get('name', 'there')}, I'd love to connect about helping {lead.get('company', 'your company')} grow."
                lead['campaign_type'] = 'fallback'
                lead['message_quality_score'] = 50
                
        self.logger.info(f"ü§ñ AI messages generated for all {len(leads)} leads")
        return leads
        
    def save_to_real_database(self, leads: List[Dict[str, Any]]) -> int:
        """Save leads to the real database with full validation"""
        self.logger.info(f"üíæ Saving {len(leads)} real leads to database...")
        
        # Ensure database exists
        os.makedirs(os.path.dirname(self.scraper_db), exist_ok=True)
        
        conn = sqlite3.connect(self.scraper_db)
        saved_count = 0
        
        # Ensure table exists with all required columns
        conn.execute("""
            CREATE TABLE IF NOT EXISTS leads (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                full_name TEXT,
                email TEXT,
                company TEXT,
                job_title TEXT,
                linkedin_url TEXT,
                industry TEXT,
                business_type TEXT,
                source TEXT,
                date_scraped TEXT,
                date_enriched TEXT,
                created_at TEXT,
                enriched INTEGER DEFAULT 0,
                ready_for_outreach INTEGER DEFAULT 0,
                score INTEGER DEFAULT 0,
                lead_quality TEXT,
                website TEXT,
                email_confidence_level TEXT,
                ai_message TEXT,
                needs_enrichment INTEGER DEFAULT 0,
                campaign_type TEXT,
                message_quality_score INTEGER
            )
        """)
        
        for lead in leads:
            try:
                # Validate lead data
                full_name = lead.get('name', '').strip()
                email = lead.get('email', '').strip()
                
                if not full_name or ' ' not in full_name:
                    self.logger.warning(f"‚ö†Ô∏è Skipping lead with invalid name: {full_name}")
                    continue
                    
                if not email or '@' not in email:
                    self.logger.warning(f"‚ö†Ô∏è Skipping lead with invalid email: {email}")
                    continue
                    
                # Check for duplicates
                cursor = conn.execute(
                    "SELECT id FROM leads WHERE full_name = ? OR email = ?",
                    (full_name, email)
                )
                
                if cursor.fetchone():
                    self.logger.warning(f"‚ö†Ô∏è Duplicate lead skipped: {full_name}")
                    continue
                    
                # Insert validated lead
                conn.execute("""
                    INSERT INTO leads (
                        full_name, email, company, job_title, linkedin_url,
                        industry, business_type, source, date_scraped, date_enriched,
                        created_at, enriched, ready_for_outreach, score, lead_quality,
                        website, email_confidence_level, ai_message, needs_enrichment,
                        campaign_type, message_quality_score
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    full_name, email, lead.get('company', ''), lead.get('job_title', ''),
                    lead.get('linkedin_url', ''), lead.get('industry', ''), 
                    lead.get('business_type', ''), lead.get('source', 'SerpAPI'),
                    lead.get('date_scraped', datetime.now().isoformat()),
                    lead.get('date_enriched', datetime.now().isoformat()),
                    datetime.now().isoformat(), 1, 1, lead.get('score', 75),
                    lead.get('lead_quality', 'Warm'), lead.get('website', ''),
                    lead.get('email_confidence_level', 'Pattern'), 
                    lead.get('ai_message', ''), 0, lead.get('campaign_type', 'personalized'),
                    lead.get('message_quality_score', 75)
                ))
                
                saved_count += 1
                self.logger.info(f"‚úÖ Saved real lead: {full_name}")
                
            except Exception as e:
                self.logger.error(f"‚ùå Failed to save lead {lead.get('name')}: {e}")
                continue
                
        conn.commit()
        conn.close()
        
        self.logger.info(f"üíæ Successfully saved {saved_count} real leads to database")
        return saved_count
        
    def sync_to_real_airtable(self, max_records: int = 10) -> Dict[str, Any]:
        """Sync real leads to Airtable"""
        self.logger.info(f"üì§ Syncing real leads to Airtable...")
        
        # Get unsynced leads from database
        conn = sqlite3.connect(self.scraper_db)
        conn.row_factory = sqlite3.Row
        
        cursor = conn.execute("""
            SELECT * FROM leads 
            WHERE enriched = 1 AND ready_for_outreach = 1 
            ORDER BY created_at DESC 
            LIMIT ?
        """, (max_records,))
        
        leads = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        if not leads:
            self.logger.info("üì§ No leads to sync")
            return {'synced': 0, 'errors': 0}
            
        synced_count = 0
        error_count = 0
        
        for lead in leads:
            try:
                # Map to Airtable format
                airtable_data = {
                    'Full Name': lead['full_name'],
                    'Email': lead['email'],
                    'Company': lead['company'],
                    'Job Title': lead['job_title'],
                    'LinkedIn URL': lead['linkedin_url'],
                    'Source': 'Search',  # Valid Airtable option
                    'Lead Quality': lead['lead_quality'],
                    'Website': lead['website'],
                    'Email_Confidence_Level': lead['email_confidence_level'],
                    'AI Message': lead['ai_message'],
                    'Date Scraped': lead['date_scraped'],
                    'Date Enriched': lead['date_enriched'],
                    'Business_Type': lead['business_type'],
                    'Company_Description': f"Real company found via SerpAPI: {lead['company']}"
                }
                
                # Create record in Airtable
                record = self.airtable.create_lead(airtable_data)
                
                if record:
                    synced_count += 1
                    self.logger.info(f"‚úÖ Synced to Airtable: {lead['full_name']}")
                else:
                    error_count += 1
                    self.logger.error(f"‚ùå Failed to sync: {lead['full_name']}")
                    
            except Exception as e:
                error_count += 1
                self.logger.error(f"‚ùå Sync error for {lead['full_name']}: {e}")
                
        self.logger.info(f"üì§ Sync complete: {synced_count} synced, {error_count} errors")
        return {'synced': synced_count, 'errors': error_count}
        
    def run_autonomous_cycle(self) -> Dict[str, Any]:
        """Run one complete autonomous cycle with REAL systems"""
        cycle_start = datetime.now()
        self.logger.info("üîÑ Starting REAL autonomous cycle...")
        
        cycle_results = {
            'start_time': cycle_start.isoformat(),
            'leads_scraped': 0,
            'leads_enriched': 0,
            'leads_saved': 0,
            'leads_synced': 0,
            'status': 'started',
            'errors': []
        }
        
        try:
            # 1. Health check
            health = self.perform_health_check()
            if health['overall_status'] == 'critical':
                cycle_results['status'] = 'aborted'
                cycle_results['errors'].append('Critical health check failure')
                return cycle_results
                
            # 2. Scrape real leads
            leads = self.scrape_real_leads(self.leads_per_cycle)
            cycle_results['leads_scraped'] = len(leads)
            
            if not leads:
                cycle_results['status'] = 'no_leads_found'
                return cycle_results
                
            # 3. Enrich real leads
            enriched_leads = self.enrich_real_leads(leads)
            cycle_results['leads_enriched'] = len(enriched_leads)
            
            # 4. Generate AI messages
            leads_with_messages = self.generate_real_ai_messages(enriched_leads)
            
            # 5. Save to database
            saved_count = self.save_to_real_database(leads_with_messages)
            cycle_results['leads_saved'] = saved_count
            
            # 6. Sync to Airtable
            sync_results = self.sync_to_real_airtable()
            cycle_results['leads_synced'] = sync_results['synced']
            
            # Calculate success
            if saved_count > 0:
                cycle_results['status'] = 'success'
            else:
                cycle_results['status'] = 'partial_success'
                
        except Exception as e:
            cycle_results['status'] = 'failed'
            cycle_results['errors'].append(str(e))
            self.logger.error(f"‚ùå Cycle failed: {e}")
            
        cycle_end = datetime.now()
        cycle_results['end_time'] = cycle_end.isoformat()
        cycle_results['duration_seconds'] = (cycle_end - cycle_start).total_seconds()
        
        self.logger.info(f"üîÑ Cycle complete: {cycle_results['status']} in {cycle_results['duration_seconds']:.1f}s")
        return cycle_results
        
    def run_forever(self, max_cycles: int = None):
        """Run the REAL organism autonomously forever"""
        self.logger.info("üß¨ REAL Autonomous Organism starting eternal life cycle...")
        
        cycle_count = 0
        total_stats = {
            'total_leads_scraped': 0,
            'total_leads_saved': 0,
            'total_leads_synced': 0,
            'successful_cycles': 0,
            'failed_cycles': 0
        }
        
        while True:
            try:
                cycle_count += 1
                
                if max_cycles and cycle_count > max_cycles:
                    self.logger.info(f"üèÅ Reached max cycles ({max_cycles}), stopping...")
                    break
                    
                self.logger.info(f"üîÑ Starting cycle #{cycle_count}")
                
                # Run autonomous cycle
                results = self.run_autonomous_cycle()
                
                # Update stats
                total_stats['total_leads_scraped'] += results['leads_scraped']
                total_stats['total_leads_saved'] += results['leads_saved']
                total_stats['total_leads_synced'] += results['leads_synced']
                
                if results['status'] == 'success':
                    total_stats['successful_cycles'] += 1
                else:
                    total_stats['failed_cycles'] += 1
                    
                # Log organism status
                self.logger.info(f"üß¨ Organism Status: {cycle_count} cycles, {total_stats['total_leads_scraped']} scraped, {total_stats['total_leads_saved']} saved, {total_stats['total_leads_synced']} synced")
                
                # Sleep until next cycle
                self.logger.info(f"üò¥ Organism resting for {self.cycle_interval//60} minutes...")
                time.sleep(self.cycle_interval)
                
            except KeyboardInterrupt:
                self.logger.info("üõë Organism stopping due to user interrupt...")
                break
            except Exception as e:
                self.logger.error(f"‚ùå Organism error in cycle {cycle_count}: {e}")
                time.sleep(300)  # Wait 5 minutes before retrying
                
        self.logger.info("üß¨ REAL Autonomous Organism lifecycle complete")
        self.logger.info(f"üìä Final Stats: {total_stats}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='REAL 4Runr Autonomous Organism')
    parser.add_argument('--test', action='store_true', help='Run a single test cycle')
    parser.add_argument('--cycles', type=int, help='Number of cycles to run (default: infinite)')
    parser.add_argument('--interval', type=int, default=10800, help='Interval between cycles in seconds (default: 3 hours)')
    
    args = parser.parse_args()
    
    # Create the REAL organism
    organism = RealAutonomousOrganism()
    organism.cycle_interval = args.interval
    
    if args.test:
        print("üß™ Running REAL organism test cycle...")
        results = organism.run_autonomous_cycle()
        print(f"üéØ Test Results: {results}")
    else:
        print("üß¨ Starting REAL autonomous organism...")
        organism.run_forever(max_cycles=args.cycles)
